# Hyperparameters follow Dabney et al. (2018), but we modify as necessary to
# match those used in Rainbow (Hessel et al., 2018), to ensure apples-to-apples
# comparison.
import dopamine.jax.agents.implicit_quantile.implicit_quantile_agent
import dopamine.discrete_domains.run_experiment
import dopamine.replay_memory.circular_replay_buffer

import dopamine.jax.agents.dqn.dqn_agent
import dopamine.jax.networks
import dopamine.discrete_domains.gym_lib

import networks_new
import implicit_quantile_agent_new

#JaxDQNAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
#JaxDQNAgent.observation_dtype = %jax_networks.CARTPOLE_OBSERVATION_DTYPE
#JaxDQNAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
#JaxDQNAgent.gamma = 0.99
#JaxDQNAgent.update_horizon = 3
#JaxDQNAgent.min_replay_history = 500 # agent steps
#JaxDQNAgent.update_period = 2
#JaxDQNAgent.target_update_period = 100 # agent step
#JaxDQNAgent.epsilon_train=0.01
#JaxDQNAgent.epsilon_eval=0.001
#JaxDQNAgent.epsilon_decay_period=250000
#JaxDQNAgent.summary_writer=None
#JaxDQNAgent.summary_writing_frequency=500

JaxImplicitQuantileAgentNew.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE
JaxImplicitQuantileAgentNew.observation_dtype = %jax_networks.CARTPOLE_OBSERVATION_DTYPE
JaxImplicitQuantileAgentNew.stack_size = %gym_lib.CARTPOLE_STACK_SIZE
JaxImplicitQuantileAgentNew.gamma = 0.99
JaxImplicitQuantileAgentNew.update_horizon = 3
JaxImplicitQuantileAgentNew.min_replay_history = 500 # agent steps
JaxImplicitQuantileAgentNew.update_period = 2
JaxImplicitQuantileAgentNew.target_update_period = 100 # agent step
JaxImplicitQuantileAgentNew.epsilon_train=0.01
JaxImplicitQuantileAgentNew.epsilon_eval=0.001
JaxImplicitQuantileAgentNew.epsilon_decay_period=250000
JaxImplicitQuantileAgentNew.summary_writer=None
JaxImplicitQuantileAgentNew.summary_writing_frequency=500


JaxImplicitQuantileAgentNew.kappa = 1.0
JaxImplicitQuantileAgentNew.num_tau_samples = 8#32
JaxImplicitQuantileAgentNew.num_tau_prime_samples =8 #32
JaxImplicitQuantileAgentNew.num_quantile_samples = 8#32
JaxImplicitQuantileAgentNew.quantile_embedding_dim = 64
JaxImplicitQuantileAgentNew.optimizer = 'adam'
JaxImplicitQuantileAgentNew.replay_scheme = 'prioritized'
JaxImplicitQuantileAgentNew.network  = @networks_new.ImplicitQuantileNetwork
JaxImplicitQuantileAgentNew.epsilon_fn = @dqn_agent.identity_epsilon #@dqn_agent.linearly_decaying_epsilon

create_optimizer.learning_rate = 0.001
create_optimizer.eps = 3.125e-4

create_gym_environment.environment_name = 'CartPole'
create_gym_environment.version = 'v0'
TrainRunner.create_environment_fn = @gym_lib.create_gym_environment

#create_runner.schedule = 'continuous_train'
#create_agent.agent_name = 'jax_implicit_quantile'
#create_agent.debug_mode = True

Runner.num_iterations = 30
Runner.training_steps = 1000
#Runner.evaluation_steps = 125000
Runner.max_steps_per_episode = 200

OutOfGraphReplayBuffer.replay_capacity = 100000#50000
OutOfGraphReplayBuffer.batch_size = 64#128
