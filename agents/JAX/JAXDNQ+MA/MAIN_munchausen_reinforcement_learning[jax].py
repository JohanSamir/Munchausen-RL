# -*- coding: utf-8 -*-
"""Munchausen-reinforcement-learning[JAX].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czajkgnDZTMzRifMAHF_j3o_poC3aCS1
"""

#You should delete the save files to run a new simulation. If this is not done this script will load the previous checkpoints and logs.

#!pip install tensorflow --upgrade


#!git clone https://github.com/kenjyoung/MinAtar.git
#%cd MinAtar
#!pip install .

#!apt install swig
#!pip install box2d box2d-kengz

!pip install -U dopamine-rl

import numpy as np
import os
#from dopamine.agents.dqn import dqn_agent

import dopamine
from dopamine.discrete_domains import run_experiment
from dopamine.colab import utils as colab_utils
from absl import flags
import gin.tf

import matplotlib
#matplotlib.use('TKAgg')

#import minatar
#minatar.__version__

#import matplotlib
#matplotlib.use('TkAgg')

from google.colab import drive 
drive.mount('/content/drive')

path = '/content/drive/My Drive/SaveFiles/Data/Dopamine_github/'
#LOG_PATH = os.path.join(path, 'dqn')
LOG_PATH = os.path.join(path, 'dqn_test')

import sys
sys.path.append(path)
from dqn_agent_new import *
#from rainbow_agent_new import *
#from quantile_agent_new import*
#from m_dqn import*

def create_random_dqn_agent(sess, environment, summary_writer=None):
#def create_random_dqn_agent(sess, environment, summary_writer=None):
  """The Runner class will expect a function of this type to create an agent."""
  print('ACTIONS',environment.action_space.n)
  return JaxDQNAgentNew(num_actions=environment.action_space.n)
  #return JaxRainbowAgentNew(num_actions=environment.action_space.n)
  #return JaxQuantileAgentNew(num_actions=environment.action_space.n)
  
  #return MunchausenDQNAgent(sess,num_actions=environment.action_space.n)
  #return MunchausenDQNAgentNew(num_actions=environment.action_space.n)
 
#gin.parse_config_file('/content/drive/My Drive/SaveFiles/Data/Dopamine_github/dqn_cartpole.gin')
#gin.parse_config_file('/content/drive/My Drive/SaveFiles/Data/Dopamine_github/rainbow_cartpole.gin')
#gin.parse_config_file('/content/drive/My Drive/SaveFiles/Data/Dopamine_github/quantile_cartpoleA.gin')
gin.parse_config_file('/content/drive/My Drive/SaveFiles/Data/Dopamine_github/dqn_cartpole.gin')
#gin.parse_config_file('/content/drive/My Drive/SaveFiles/Data/Dopamine_github/dqn_acrobot.gin')

random_dqn_runner = run_experiment.TrainRunner(LOG_PATH, create_random_dqn_agent)

print('Will train agent, please be patient, may be a while...')
random_dqn_runner.run_experiment()
print('Done training!')

data = colab_utils.read_experiment(LOG_PATH, verbose=True, summary_keys=['train_episode_returns'])
data['agent'] = 'Breakout'
data['run'] = 1

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(16,8))
sns.lineplot(x='iteration', y='train_episode_returns',   data=data, ax=ax)
plt.title('Cartpole')
plt.show()

cp = np.array([[-1.2355818, -1.9407905],[-1.8954086, -1.8017461], [-1.1788838 ,-1.7149097]])
fc = cp/1
print(fc.shape)

print(fc)

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

e = tf.nn.softmax(fc ,axis=-1)
b = tf.print('e: ', e)

with tf.control_dependencies([b]):
  e = tf.identity(e)

print('Non-interactive:')
with tf.Session() as sess:
  sess.run(e)

r = jax.nn.softmax(fc, axis=1)
print(r)

